pattern_name,workload_characteristics,data_access_pattern,storage_requirements,compute_requirements,separation_benefits,architecture_approach,typical_latency,data_freshness_requirement,consistency_model,storage_format,caching_strategy,example_use_cases,cost_optimization
Batch Training Pipeline,Large dataset processing,Sequential scan of training data,"High throughput, cost-effective","GPU/TPU intensive, elastic scaling","Cost optimization, independent scaling",Data lake + compute clusters,Hours to days,Batch updates,Eventually consistent,"Parquet, Delta, Iceberg",Distributed caching during training,"Large language model training, computer vision","Spot instances, storage tiering"
Online Feature Store,Real-time feature serving,Point lookups by entity ID,"Low latency, high availability","Low-latency serving, auto-scaling","Feature reuse, independent updates",Cache + backing store separation,< 10ms,Near real-time,Read-committed with versioning,Row-based with indexing,"Multi-tier caching (Redis, local)","Recommendation systems, fraud detection",Hot/warm/cold tiering
Streaming ML Pipeline,Continuous model updates,Streaming ingestion + batch reads,Stream processing + historical storage,Stream processing + periodic retraining,Independent stream and batch processing,Lambda architecture with separation,Seconds to minutes,Real-time streaming,Stream ordering + eventual batch,Streaming + columnar storage,Stream buffering + batch caching,"Real-time personalization, anomaly detection",Stream processing right-sizing
Vector Similarity Search,High-dimensional vector operations,Nearest neighbor searches,Efficient vector storage and indexing,"Vector processing, GPU acceleration",Vector index optimization separate from storage,Specialized vector stores + compute,< 100ms,Configurable (real-time to batch),Index consistency with base data,"Vector-optimized formats (FAISS, Annoy)",Vector index caching,"Semantic search, image recognition","Index compression, approximate search"
Model Inference at Scale,High-throughput model serving,Batch and real-time inference requests,Model artifact storage,Inference-optimized compute (CPU/GPU),Model versioning independent of compute,Model registry + serving infrastructure,Milliseconds to seconds,Model version consistency,Model versioning consistency,"Model formats (ONNX, TensorFlow, PyTorch)","Model caching, result caching","API serving, batch prediction","Auto-scaling, model compression"
Experimental ML Workbench,Ad-hoc analysis and experimentation,"Interactive queries, sample data access","Flexible data access, cost-effective","On-demand compute, notebook environments","Cost control, resource isolation",Notebook + data lake separation,Seconds to minutes,"Flexible, often historical","Read-committed, snapshot isolation","Multiple formats (CSV, Parquet, JSON)",Session-based caching,"Data science exploration, prototyping","Spot instances, data sampling"
MLOps Pipeline,End-to-end ML lifecycle management,"Mixed: training, validation, monitoring",Versioned data and model artifacts,"Pipeline orchestration, various compute types",Independent component scaling and versioning,Orchestrated pipeline with separated components,Variable by pipeline stage,Pipeline-dependent,Pipeline consistency guarantees,Multi-format with versioning,Pipeline-stage specific caching,"Production ML systems, A/B testing","Pipeline optimization, resource scheduling"
